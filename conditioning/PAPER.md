# Classical Conditioning as a Pure Functional Fold: Bayesian Rate Estimation in GenJAX

## Abstract

We present a purely functional implementation of the Gershman (2025)
rate estimation model of classical conditioning, in which an
organism's entire history of stimulus exposure, rate learning, and
conditioned responding is expressed as a stateless fold over a
sequence of timestep events. The model treats Pavlovian conditioning
as Bayesian rate estimation of Poisson processes: each stimulus
excites a latent rate, reinforcements are generated by the
superposition of these rates, and conditioned responding is triggered
by the information gain of the CS relative to the background. Our
implementation in JAX and GenJAX eliminates all mutable state: the
organism's knowledge is a vector of rate estimates threaded through
`lax.scan`, the learning rule is a single arithmetic expression per
timestep, and all conditioning phenomena arise from different stimulus
configurations fed to the same fold. We reproduce all figures from the
paper — including the Rescorla-Wagner model's failures, proportional
estimation error convergence, timescale invariance with empirical data
overlay, and informativeness-based model comparison across three
experimental datasets — and demonstrate three additional phenomena
(spacing effect, contingency degradation, extinction/reacquisition)
that emerge from the same organism without modification.

---

## 1. Introduction

Classical conditioning is the process by which an organism learns that
one stimulus (the conditioned stimulus, CS) predicts another (the
unconditioned stimulus, US). Despite a century of research, the
computational principles underlying conditioning remain debated. Two
views dominate: the *associative* view, in which organisms form
stimulus-response associations updated by prediction errors, and the
*representational* view, in which organisms estimate the statistical
structure of the stimulus environment and derive responses from that
structure.

The Rescorla-Wagner model (Rescorla & Wagner, 1972) is the canonical
associative theory. It updates associative weights by an error-driven
rule:

```
w_i(n+1) = w_i(n) + alpha * x_i(n) * delta(n)
```

where `delta(n) = r(n) - sum_j w_j(n) * x_j(n)` is the prediction
error. The conditioned response is assumed to be a monotonic function
of the US prediction `r_hat = sum_i w_i * x_i`. This model is
elegant and neurobiologically plausible, but it fails on two critical
empirical phenomena: the *spacing effect* (longer intertrial intervals
produce faster acquisition) and *contingency degradation* (adding
unsignaled US deliveries during the intertrial interval reduces
conditioned responding). Both manipulations leave the CS-conditional
reinforcement rate unchanged, and the Rescorla-Wagner model's
asymptotic US prediction depends only on this rate.

Rate Estimation Theory (RET; Gallistel, 1990; Gallistel & Gibbon,
2000) offers a representational alternative. It posits that organisms
estimate the rate of reinforcement associated with each stimulus, and
respond based on the *relative* rate — the information gain of
observing the CS:

```
Delta_H = log((lambda_CS + lambda_B) / lambda_B)
```

where `lambda_CS` and `lambda_B` are the CS-conditional and
background reinforcement rates. This relative response rule explains
spacing (longer ITI reduces `lambda_B`) and contingency degradation
(extra US during ITI increases `lambda_B`), because both manipulations
change the background rate while the absolute response rule of
Rescorla-Wagner is insensitive to it.

Gershman (2025) made a surprising connection between these two views.
He showed that a Bayesian approach to rate estimation yields a
learning rule that closely resembles the Rescorla-Wagner update — the
same error-driven, stimulus-gated form — but operating on *rate
estimates* rather than associative weights, with a *decreasing
learning rate* that reflects accumulated evidence. The critical
difference is not in the learning rule but in the *response rule*:
relative (information gain) rather than absolute (associative
strength). Combined with a Gamma-Poisson conjugate prior, this yields
an analytically tractable model that explains timescale invariance —
the empirical law that reinforcements to acquisition is inversely
proportional to the informativeness of the protocol (C/T) on a log
scale.

### 1.1 The Organism as a Fold

Gershman's reference implementation uses a Python class with mutable
instance variables (`self.lambda_hat`, `self.N`) and continuous-time
numerical integration via `scipy.integrate.quad_vec`. Each call to
`model.run()` mutates the object's internal state, and experimental
simulations accumulate results through imperative loops.

We re-implement this model as a purely functional fold:

```
(prior, events) ──> fold ──> (rate_estimates, decision_variables, deltas)
```

The fold is a `jax.lax.scan` over the event sequence. The carry
contains the organism's current state: a vector of rate estimates and
a scalar time counter. Each step computes the prediction error,
updates the rates, and emits the decision variable. There is no
mutable state outside the scan — the organism is a pure function from
stimulus history to behavioral output.

This is the second organism in our series, following the habituation
model (Gershman, 2024) implemented as a GP filtering fold. While the
habituation organism's memory was a masked kernel matrix with `None`
carry (each step re-solved the full GP posterior), the conditioning
organism's memory is a compact state vector threaded through the
carry. Both are pure folds; they differ in what constitutes the
organism's "sufficient statistic" of experience.

### 1.2 Contributions

1. A `lax.scan`-based implementation of Bayesian rate estimation,
   replacing an OOP class with continuous-time integration by a
   discrete-time pure fold.
2. Reproduction of all figures from Gershman (2025): Rescorla-Wagner
   failures (Figs 2-3), proportional estimation error (Fig 4),
   timescale invariance with empirical data overlay (Fig 5), and
   informativeness with real-data curve fitting and BIC model
   comparison across three datasets (Fig 6).
3. Three additional demonstrations — spacing effect, contingency
   degradation, and acquisition/extinction/reacquisition — showing
   phenomena that emerge from the same organism without modification.
4. A GenJAX generative model with Gamma priors and Poisson
   observations, validated against the analytical Gamma-Poisson
   conjugate posterior via importance sampling.

---

## 2. Methods

### 2.1 The Generative Model

The organism assumes the world generates reinforcements according to
a stimulus-dependent superposition of Poisson processes:

```
Stimuli:       x_i(t) in {0, 1}     presence of stimulus i at time t
Rates:         lambda_i ~ Gamma(r_0, n_0)    prior on each rate
Reinforcement: r(t) ~ Poisson(sum_i lambda_i * x_i(t) * dt)
```

The background stimulus (index B) is always present: `x_B(t) = 1`
for all t. The CS is present during the interstimulus interval and
absent during the intertrial interval. In standard delay conditioning,
exactly one US is delivered at the end of each CS presentation.

The organism's learning problem is to estimate the rates `lambda_i`
from the observed stimulus process `x(t)` and reinforcement process
`r(t)`. Its response problem is to decide when the CS provides
information about upcoming reinforcement.

The response rule is the log information gain (Eq. 15 in the paper):

```
Decision variable:  Delta_H = log((lambda_CS + lambda_B) / lambda_B)
Response criterion: Delta_H > log(beta)
```

This measures how much the CS reduces uncertainty about the time to
the next reinforcement, relative to the background alone.

### 2.2 The Learning Rule

Gershman (2025) derives the learning rule from approximate Bayesian
estimation of Poisson rates. The key equations are:

```
Prediction error:   delta(t) = r(t) - sum_j lambda_hat_j(t) * x_j(t)
Effective duration: N'_i(t)  = eta * t + n_0
Learning rate:      alpha(t) = dt / N'_i(t)
Update:             lambda_hat_i(t+dt) = lambda_hat_i(t) + alpha(t) * x_i(t) * delta(t)
Initialization:     lambda_hat_i(0) = r_0 / n_0
```

This is structurally identical to the Rescorla-Wagner update (Eq. 2),
with three differences:

1. The "weights" are rate estimates, not associative strengths.
2. The learning rate decreases as `1/(eta * t + n_0)`, reflecting
   the Bayesian compromise between prior and evidence.
3. The initialization is the prior mean `r_0/n_0`, not zero.

The parameter `eta` interpolates between fully independent learning
rates (one per stimulus, based on its own exposure time) and a shared
learning rate (based on total elapsed time). In practice, Gershman
fixes `eta = 0.7` across all stimuli.

Default parameters from the paper: `r_0 = 0.1`, `n_0 = 1.0`,
`eta = 0.7`, `dt = 0.5` (500 ms bins).

### 2.3 Architecture: The Organism as a Pure Function

The system is composed from pure functions across three layers:

```
+---------------------------------------------------------------+
|                      LAYER 3: EXPERIMENTS                      |
|                                                                |
|  sim_rw_spacing    sim_timescale_invariance    sim_extinction   |
|       |                     |                       |          |
|       +--- build events ----+--- build events ------+          |
|              |                       |                          |
+---------------------------------------------------------------+
|                      LAYER 2: ORGANISM LIFETIME                |
|                                                                |
|      compute_conditioning_response    compute_rw_response      |
|                  |                            |                |
|            jax.lax.scan                 jax.lax.scan            |
|            over events                  over events             |
|                  |                            |                |
+---------------------------------------------------------------+
|                      LAYER 1: SINGLE-STEP UPDATE               |
|                                                                |
|   prediction_error --> learning_rate --> rate_update            |
|          |                   |               |                 |
|    delta = r - r_hat    dt / N'(t)    lambda += alpha*x*delta  |
|                                              |                 |
|                                     decision_variable          |
|                                  log(lambda_total / lambda_bg) |
|                                                                |
+---------------------------------------------------------------+
```

**Layer 1** is the single-step update — pure arithmetic on arrays:

- Prediction error: `delta = r - dot(lambda_hat, x)`
- Learning rate: `alpha = dt / (eta * t + n_0)`
- Rate update: `lambda_hat_new = lambda_hat + alpha * x * delta`
- Decision variable: `log(sum(lambda_hat * x) / lambda_hat_bg)`

**Layer 2** is the organism's lifetime, expressed as `lax.scan`:

```
events:    [ e_0,   e_1,   e_2,  ...,  e_{T-1}  ]
              |      |      |            |
              v      v      v            v
scan:      [ step,  step,  step, ...,  step      ]
              |      |      |            |
              v      v      v            v
outputs:   [ (lambda_0, dv_0, delta_0), ..., (lambda_{T-1}, dv_{T-1}, delta_{T-1}) ]
```

**Layer 3** contains the experimental simulations, which construct
event sequences (stimulus schedules) and feed them to the organism.

### 2.4 The Scan Body: One Timestep of the Organism

The conditioning organism, unlike the habituation organism, has
non-trivial carry state. The carry contains `(lambda_hat, t)` — the
current rate estimates and the elapsed time. Each step reads one
event `[x_CS, x_B, r]`, updates the state, and emits outputs:

```
step : (carry=(lambda_hat, t), event=[x_CS, x_B, r])
     -> (carry=(lambda_hat_new, t_new), output=(lambda_hat_new, dv, delta))

  1.  x         = event[:n_stimuli]        # stimulus presence vector
  2.  r         = event[n_stimuli]          # reinforcement (0 or 1)
  3.  r_hat     = dot(lambda_hat, x)        # predicted reinforcement
  4.  delta     = r - r_hat                 # prediction error
  5.  N_prime   = eta * t + n_0             # effective observation period
  6.  alpha     = dt / N_prime              # learning rate
  7.  lambda_new = lambda_hat + alpha * x * delta   # rate update
  8.  lambda_new = max(lambda_new, 1e-10)   # positivity constraint
  9.  total     = sum(lambda_new * x)       # total rate given stimuli
  10. bg        = lambda_new[-1]            # background rate
  11. dv        = log(total / bg)           # decision variable
  12. t_new     = t + dt                    # advance clock

  return (lambda_new, t_new), (lambda_new, dv, delta)
```

This is 12 lines of pure arithmetic. Compare with the reference
implementation's class-based approach:

```
REFERENCE (class + scipy)           OURS (lax.scan)
-------------------------           ----------------

class model_constructor:            def step(state, event):
  def __init__(self, ...):            lambda_hat, t = state
    self.lambda_hat = ...    <-- mut  x = event[:n_stimuli]
    self.N = ... + n0        <-- mut  r = event[n_stimuli]
                                      r_hat = dot(lambda_hat, x)
  def predict(self, x):              delta = r - r_hat
    return dot(x, self.lambda_hat)   N_prime = eta * t + n_0
                                      alpha = dt / N_prime
  def run(self, events, ...):         lambda_new = lambda_hat + alpha * x * delta
    steps = arange(t_start, t_end)   lambda_new = max(lambda_new, 1e-10)
    for i in range(len(steps)-1):    ...
      self.N += step_size*eta <-- mut return (lambda_new, t_new), outputs
      delta, _ = quad_vec(    <-- scipy
        update, steps[i], steps[i+1])
      self.lambda_hat += delta <-- mut
      self.lambda_hat = fmax(...)

  def update(self, x, r):
    return divide(x, self.N) * (r - self.predict(x))
```

The left column has three sources of mutation: `self.lambda_hat`,
`self.N`, and the implicit state of `quad_vec`. The right column is
a pure function — input in, output out, no side effects.

### 2.5 Event Sequences: Building the World

The organism does not control its environment; it receives a
pre-built event sequence. Each event is a vector
`[x_CS, x_B, r]` encoding stimulus presence and reinforcement at one
timestep. Different experimental protocols are different event
sequences — the organism is the same.

```
Standard delay conditioning (T=ISI, I=ITI, dt=0.5):

  Trial 1                    Trial 2
  |<--- ISI --->|<--- ITI --->|<--- ISI --->|<--- ITI --->|
  CS on, BG on   CS off, BG on CS on, BG on  CS off, BG on
  r=0 ... r=1    r=0 ... r=0  r=0 ... r=1   r=0 ... r=0

  Event array (each row = one dt step):
  [ [1, 1, 0],   # CS period, no US
    [1, 1, 0],
    [1, 1, 0],
    [1, 1, 1],   # US delivered at CS offset
    [0, 1, 0],   # ITI period, no US
    [0, 1, 0],
    ...
    [1, 1, 0],   # Next trial begins
    ... ]
```

We provide several event builders:

| Builder | Use | Key parameters |
|---|---|---|
| `build_delay_conditioning_events` | Rate estimation model | n_trials, ISI, ITI, dt, extra_us |
| `build_rw_delay_events` | Rescorla-Wagner comparison | n_trials, ISI, ITI, binsize, degrade |
| `generate_poisson_events` | Poisson process validation | ISI, ITI, lambda_bg, lambda_cs, total_time |

The Rescorla-Wagner builder uses 1-second bins (matching Gershman's
reference code), while the rate estimation builder uses the default
`dt = 0.5`. The Poisson builder generates stochastic reinforcements
sampled from the true generative process, used to validate that the
learning algorithm recovers the true rates.

### 2.6 The Rescorla-Wagner Comparison Model

To demonstrate why the rate estimation model succeeds where the
Rescorla-Wagner model fails, we implement the RW model as a second
`lax.scan`:

```
step_rw : (carry=w, event=[x_CS, x_B, r])
        -> (carry=w_new, output=(w_new, r_hat))

  1.  x     = event[:n_stimuli]
  2.  r     = event[n_stimuli]
  3.  r_hat = dot(w, x)               # US prediction
  4.  delta = r - r_hat                # prediction error
  5.  w_new = w + alpha_lr * x * delta # weight update

  return w_new, (w_new, r_hat)
```

This is the same structure as the rate estimation step, but with a
fixed learning rate and zero initialization. The critical output for
plotting is `r_hat` — the US prediction at the moment of US delivery,
which is what the Rescorla-Wagner model uses as its response measure.

The RW model supports per-stimulus learning rates via an
`alpha_lr` parameter that can be a scalar or a per-stimulus array,
allowing us to test the hypothesis that different learning rates for
CS and context might rescue the model (they do not).

### 2.7 The GenJAX Generative Model

The probabilistic structure is made explicit in a GenJAX program:

```
@gen
conditioning_rate_model(stimulus_record, dt, r_0, n_0):
    |
    +-- lambda_cs ~ Gamma(r_0, n_0)          @ "lambda_cs"
    |     |
    |     +-- CS reinforcement rate
    |
    +-- lambda_bg ~ Gamma(r_0, n_0)          @ "lambda_bg"
    |     |
    |     +-- background reinforcement rate
    |
    +-- rates = (lambda_cs * x_CS + lambda_bg * x_BG) * dt
    |
    +-- obs ~ Poisson(rates)                  @ "obs"
    |     |
    |     +-- observed reinforcement counts per time bin
    |
    +-- return (lambda_cs, lambda_bg)
```

This is a Gamma-Poisson conjugate model. The prior on each rate is
`Gamma(r_0, n_0)`, and the likelihood of observing reinforcement
counts is Poisson with rate determined by the stimulus-weighted sum
of rates multiplied by `dt`. The conjugate posterior is
`Gamma(r_0 + R_i, n_0 + N_i)` where `R_i` is the total
reinforcement count attributed to stimulus `i` and `N_i` is its
total exposure time.

### 2.8 Validation: IS vs Analytical Posterior

For a single-stimulus scenario, the Gamma-Poisson conjugacy gives an
exact posterior. We validate the GenJAX model by comparing importance
sampling estimates against this closed form:

```
Analytical posterior:  Gamma(r_0 + R, n_0 + N)
  where R = total reinforcements, N = total exposure * dt

IS procedure:
  for each sample:
    lambda ~ Gamma(r_0) / n_0       # sample from prior
    rates = lambda * x * dt
    log_w = sum(Poisson(rates).log_prob(obs))   # likelihood weight
  posterior_mean = weighted_mean(lambda, normalize(exp(log_w)))
```

Using `r_0 = 2.0`, `n_0 = 1.0`, 5 timesteps with 3 reinforcements,
and 50,000 importance samples, the IS posterior mean matches the
analytical posterior to within 0.002.

### 2.9 Data Flow: End-to-End

The complete pipeline from parameters to figures, with no mutable
state until the I/O boundary:

```
 PARAMETERS              EVENTS                    OUTPUTS
 ----------              ------                    -------
 r_0=0.1           events: (T, 3)
 n_0=1.0    -----+  [x_CS, x_B, r]
 eta=0.7          |       |
 dt=0.5           v       v
             +------------------------+
             |  compute_conditioning  |
             |  _response             |
             |  +------------------+  |
             |  | prediction error |  |
             |  | learning rate    |  |
             |  | rate update      | xT |--- lambda_hats: (T, K)
             |  | decision var     |  |     dvs:         (T,)
             |  +------------------+  |     deltas:      (T,)
             +------------------------+
                        |
                        v
             +------------------------+
             |  extract per-trial     |--- per-trial values
             |  values at US indices  |
             +------------------------+
                        |
                        v
             +------------------------+
             |  matplotlib            |--- figure (side effect,
             |  (I/O boundary)        |    at boundary only)
             +------------------------+
```

Everything from parameters to per-trial values is a pure function
composition. Side effects exist only at the outermost boundary.

---

## 3. Results

### 3.1 Rescorla-Wagner Model Failures

**Figure 2 — Spacing and contingency invariance.** We simulate the
Rescorla-Wagner model with equal learning rates (alpha = 0.1) over
80 trials of standard delay conditioning (ISI = 2s, ITI = 5s or
10s). The left panel shows the US prediction `r_hat` at CS offset as
a function of trial. The two ITI conditions produce nearly identical
learning curves — the Rescorla-Wagner model cannot explain the
spacing effect. The right panel shows contingent versus degraded
contingency (an extra US delivered during the ITI). Again, the curves
are nearly identical — the model cannot explain contingency
degradation.

This occurs because the RW model's asymptotic US prediction equals
`R_CS / T` — the CS-conditional reinforcement rate — which is
independent of the ITI and independent of whether additional US
deliveries occur during the ITI.

**Figure 3 — Different learning rates fail too.** Even with a lower
learning rate for the background (alpha_bg = 0.01, alpha_cs = 0.1),
the RW model produces overlapping curves for different ITIs. For
contingency degradation, the curves separate slightly, but in the
*wrong direction*: the degraded condition shows faster learning, not
slower. This is because the extra ITI reinforcement increases the
background weight, which adds to the total US prediction rather than
subtracting from it.

### 3.2 Rate Estimation Model

**Figure 4 — Estimation error convergence.** Reinforcements are
generated from a true Poisson process with `lambda_bg = 0.5` and
`lambda_cs = 1.5` (ISI = 2s, ITI = 5s). The proportional estimation
error `|lambda_hat - lambda| / lambda` for both background and CS
rates is plotted over 25,000 seconds of simulated time. Both errors
start high (around 0.7-0.8) and converge toward zero, demonstrating
that the approximate Bayesian learning rule correctly recovers the
true generative parameters.

The learning curve begins to asymptote after approximately 5,000
seconds (roughly 700 trials), consistent with the paper's report.
The residual floor (~0.07) in our discrete-time approximation reflects
the `dt = 0.5` discretization; Gershman's continuous-time integration
converges closer to zero.

**Figure 5 — Timescale invariance.** The log-transformed decision
variable is plotted over 80 trials of standard delay conditioning for
three interstimulus intervals (ISI = 4, 8, 16 seconds).

Left panel (fixed ITI = 48s): Shorter ISIs produce faster growth of
the decision variable, because higher informativeness (C/T ratio)
means the CS is more informative relative to the background. Data
points from Gibbon et al. (1977) pigeon autoshaping experiments are
overlaid — the decision variable curves intersect these empirical
acquisition times at roughly the same threshold value across
conditions, consistent with a fixed decision threshold `beta`.

Right panel (fixed informativeness = 6): The ITI is rescaled so that
C/T = 6 for all conditions. All three curves superimpose — the
decision variable trajectory is timescale invariant. The empirical
data points cluster together, confirming that acquisition speed
depends on the C/T ratio, not absolute timing.

**Figure 6 — Informativeness predicts learning speed.** Three
experimental datasets (Gibbon & Balsam, 1981; Balsam et al., 2024;
Harris & Gallistel, 2024) are plotted on log-log scales showing
reinforcements to acquisition (R*) versus informativeness (C/T). Two
models are fit to each dataset:

- `R* = k / (C/T - 1)` — the Gallistel & Harris (2024) law
- `R* = k / (C/T)` — the new law derived from Gershman's model (Eq. 29)

BIC model comparison decisively favors the `C/T` law across all three
datasets (posterior probability > 0.99), confirming the paper's
analytical prediction that `R* ~ k * (C/T)^{-1}`.

### 3.3 Extended Demonstrations

These figures are not in the paper but demonstrate additional
phenomena that emerge from the same organism:

**Figure 7 — Spacing effect.** The rate estimation model is run with
three ITI values (2, 5, 10) and identical ISI. Longer ITI produces
faster growth of the decision variable, because more background-only
time without reinforcement drives `lambda_B` down, increasing the
information gain `log((lambda_CS + lambda_B) / lambda_B)`. This is
exactly the phenomenon the Rescorla-Wagner model cannot capture.

**Figure 8 — Contingency degradation.** Adding an extra US during the
ITI increases `lambda_B`, which reduces the information gain. The
degraded condition shows markedly slower growth of the decision
variable compared to the contingent condition — the opposite of what
the Rescorla-Wagner model (incorrectly) predicts.

**Figure 9 — Acquisition, extinction, and reacquisition.** Three
phases are run sequentially through the same organism:

1. **Acquisition** (20 trials of CS+US): CS rate estimate rises.
2. **Extinction** (20 trials of CS without US): CS rate declines as
   the organism learns that the CS no longer predicts the US.
3. **Reacquisition** (10 trials of CS+US after a gap): CS rate rises
   again.

The key observation is that the organism maintains continuity across
phases — the rate estimates from acquisition are not reset but
gradually adjusted by extinction. This is a natural consequence of
the `lax.scan` threading state across the full event sequence.

### 3.4 Validation

Importance sampling from the GenJAX generative model matches the
analytical Gamma-Poisson conjugate posterior:

```
  Metric              Value
  ------              -----
  Analytical mean     0.8333
  IS mean             0.8315
  Mean error          0.0018
  Analytical var      0.1389
  IS var              0.1386
  Var error           0.0003
  n_samples           50,000
  Result              PASS (error < 0.05)
```

---

## 4. Discussion

### 4.1 One Organism, Many Phenomena

The most important result is that all conditioning phenomena — RW
failures, rate convergence, timescale invariance, spacing,
contingency degradation, extinction — are produced by the same
12-line scan body operating on different event sequences. There is no
parameter switching, no mode flag, no phenomenon-specific mechanism.
The organism is a single pure function:

```
organism : (r_0, n_0, eta, dt) -> (events -> (lambdas, dvs, deltas))
```

It is a curried function. Given its "biology" (prior parameters and
learning rate), it returns a function from event sequences to
behavioral histories. The explanatory power comes from two sources:
the *structure of the update rule* (Bayesian rate estimation) and the
*structure of the response rule* (relative information gain). Neither
alone is sufficient; together, they produce the full repertoire.

```
  organism(biology)(delay_conditioning_events)    = acquisition
  organism(biology)(spacing_events)               = spacing_effect
  organism(biology)(degraded_events)              = contingency_degradation
  organism(biology)(extinction_events)            = extinction
  organism(biology)(poisson_events)               = rate_convergence
```

### 4.2 Comparison with the Habituation Organism

This is the second organism in our series. The structural parallels
with the habituation model (Gershman, 2024) are informative:

```
                    HABITUATION              CONDITIONING
                    (GP filter)              (Rate estimator)
                    -----------              ----------------

Core function:      compute_habituation      compute_conditioning
                    _response                _response

Scan over:          timesteps (0..T)         events ([x, r] sequence)

Carry state:        None                     (lambda_hat, t)
                    (stateless!)             (rate vector + clock)

Memory:             Masked kernel matrix     Rate estimate vector
                    (implicit in mask index) (explicit in carry)

Per-step cost:      O(T^2) Cholesky          O(K) arithmetic
                                             (K = number of stimuli)

Observation model:  x ~ N(x_bar, alpha)      r ~ Poisson(sum lambda*x)

Response rule:      Phi((x_hat-psi)/sigma)   log((lambda_CS+lambda_B)/lambda_B)

GenJAX model:       MVN prior + MVN obs      Gamma prior + Poisson obs

Validation:         IS vs GP posterior       IS vs Gamma-Poisson conjugate
```

The habituation organism has no carry — its "memory" is the set of
unmasked entries in a pre-computed kernel matrix, accessed via the
timestep index. The conditioning organism has a compact carry — a
K-dimensional rate vector and a scalar clock. Both are valid
functional folds; they differ in whether the sufficient statistic of
experience is encoded structurally (in which entries are unmasked) or
explicitly (in the carry values).

The habituation organism is O(T^3) per step (Cholesky of a T x T
matrix). The conditioning organism is O(K) per step (dot product and
elementwise operations on a K-dimensional vector). This difference
reflects the underlying probabilistic structure: the GP has
long-range temporal correlations that require the full covariance
matrix, while the Poisson rate estimator's sufficient statistics are
incrementally updateable.

### 4.3 The Key Insight: Learning vs Response

Gershman's central argument is that the Rescorla-Wagner model's
failure is not in its *learning rule* but in its *response rule*.
The learning rules are structurally identical:

```
RW:    w_i     += alpha       * x_i * (r - dot(w, x))
Rate:  lambda_i += (dt/N'(t)) * x_i * (r - dot(lambda, x))
```

Both are error-driven, stimulus-gated updates. The difference is:

1. **Learning rate**: Fixed `alpha` vs decreasing `dt/N'(t)`.
2. **Initialization**: Zero vs prior mean `r_0/n_0`.
3. **Response rule**: Absolute `r_hat = dot(w, x)` vs relative
   `dv = log(sum(lambda*x) / lambda_B)`.

The decreasing learning rate is a consequence of Bayesian estimation
— it reflects the organism's growing confidence. The non-zero
initialization is a consequence of the Gamma prior. But the decisive
factor is the response rule. The absolute rule `r_hat` depends only
on the CS-conditional rate; the relative rule `dv` compares it to
the background rate. This comparison is what makes the model
sensitive to spacing (which affects `lambda_B` through ITI exposure)
and contingency degradation (which affects `lambda_B` through ITI
reinforcement).

### 4.4 Continuous vs Discrete Time

One architectural difference from the reference code deserves
comment. Gershman uses `scipy.integrate.quad_vec` for continuous-time
integration of the update equation, while we use discrete-time
`lax.scan` with `dt = 0.5`. This introduces a small discretization
error visible in Figure 4, where our proportional estimation error
converges to ~0.07 rather than ~0. The discretization has no effect
on the qualitative phenomena (all figures reproduce correctly) and is
a deliberate trade-off: discrete-time `lax.scan` enables JIT
compilation, GPU acceleration, and composition with GenJAX, none of
which are available for `scipy.integrate.quad_vec`.

The paper itself notes that "updating happens at regular 500 ms
intervals" in the simulations, so the discrete approximation is
consistent with the stated model.

### 4.5 The Event Sequence as World Model

An elegant aspect of the functional architecture is the separation
between *organism* and *world*. The organism is the scan body plus
its parameters. The world is the event sequence. Different
experimental protocols are different event sequences:

```
Delay conditioning:     build_delay_conditioning_events(n_trials, ISI, ITI, dt)
Contingency degradation: build_delay_conditioning_events(..., extra_us=True)
Poisson world:          generate_poisson_events(ISI, ITI, lambda_bg, lambda_cs, ...)
Extinction:             concatenate(acquisition_events, extinction_events, gap, test)
```

The organism does not know which protocol it is experiencing. It
simply processes events one at a time. The fact that spacing produces
faster learning, contingency degradation produces slower learning,
and extinction produces rate decline are all emergent properties of
the interaction between the organism's update rule and the event
statistics — not hard-coded behavioral rules.

This separation also makes it trivial to test the organism in novel
environments. Want to know what happens with partial reinforcement?
Build an event sequence where US is delivered on only 50% of trials.
Want compound conditioning with two CSs? Extend the event vector to
`[x_CS1, x_CS2, x_B, r]` and set `n_stimuli = 3`. The organism
generalizes without modification because the scan body operates on
arbitrary-length stimulus vectors.

### 4.6 The Role of the GenJAX Model

The GenJAX generative model serves the same dual purpose as in the
habituation paper. First, it makes the probabilistic assumptions
explicit: the Gamma prior over rates and the Poisson observation
model are written as sampling statements, not implicit in an update
equation. Second, it enables validation against the analytical
posterior, confirming that the learning rule is a correct
approximation of Bayesian inference.

The Gamma-Poisson conjugacy means the posterior is available in
closed form: `Gamma(r_0 + R, n_0 + N)`. This is simpler than the
habituation model's GP posterior (which requires matrix inversion)
and correspondingly easier to validate. The IS validation passes with
a mean error of 0.0018 — well within the 0.05 threshold.

### 4.7 Timescale Invariance as an Analytical Result

A notable feature of Gershman's model is that timescale invariance
is not merely an empirical observation but an analytical consequence.
From Eq. 29 in the paper:

```
R* ~ (r_0 * (beta - 2) / eta) * (C/T)^{-1}
```

The term multiplying `(C/T)^{-1}` does not depend on the
experimental protocol (ISI, ITI), so it acts as a universal constant
`k`. This predicts a linear relationship between `log(R*)` and
`log(C/T)` with slope -1. Our Figure 6 confirms this prediction
across three independent datasets spanning different species,
response measures, and ranges of informativeness.

The BIC comparison between `R* = k/(C/T)` and `R* = k/(C/T - 1)`
consistently favors the former, which is the prediction of
Gershman's model. The latter corresponds to the older formulation
`R* = k/(I/T)`, which is equivalent to `k/(C/T - 1)` since
`I/T = C/T - 1`. The empirical distinction between these two
functional forms is subtle (they diverge mainly at low
informativeness), but the data consistently favor the `C/T` form.

---

## 5. Conclusion

We have shown that classical conditioning — one of the most
extensively studied phenomena in all of psychology — can be
implemented as a pure functional fold over a sequence of
stimulus-reinforcement events. The fold body is 12 lines of
arithmetic. It has no hidden state beyond a rate vector and a
clock. It requires no phenomenon-specific mechanisms, no mode
switches, no special cases.

The organism produces:

- Correct rate estimation from Poisson processes (Fig 4)
- Timescale invariance matching three empirical datasets (Figs 5-6)
- Spacing effects that the Rescorla-Wagner model cannot capture (Fig 7)
- Contingency degradation effects (Fig 8)
- Acquisition, extinction, and reacquisition dynamics (Fig 9)
- Correct identification of the functional form of the
  informativeness law via BIC model comparison (Fig 6)

All from the same function, applied to different input sequences.

Together with the habituation organism, this demonstrates a pattern:
the organism-as-fold is a general architectural principle for
implementing Bayesian models of learning. The habituation organism is
a GP fold with O(T^3) cost per step and implicit (mask-based) memory.
The conditioning organism is a rate-estimation fold with O(K) cost
per step and explicit (carry-based) memory. Both are pure, both are
composable, and both produce rich behavioral repertoires from minimal
computational primitives.

The Rescorla-Wagner model and Bayesian rate estimation share the same
update structure — error-driven, stimulus-gated learning. The
difference is in the response rule: absolute versus relative. This
suggests that the bridge between associative and representational
theories of learning is shorter than it appears. The learning
mechanisms may be the same; what differs is how the organism uses
what it has learned.

---

## References

- Balsam, P. D., Simpson, E. H., Taylor, K., Kalmbach, A., &
  Gallistel, C. R. (2024). Learning depends on the information
  conveyed by temporal relationships between events and is reflected
  in the dopamine response to cues. *Science Advances*, 10, eadi7137.
- Gallistel, C. R. (1990). *The Organization of Learning*. MIT Press.
- Gallistel, C., & Gibbon, J. (2000). Time, rate, and conditioning.
  *Psychological Review*, 107(2), 289-344.
- Gershman, S. J. (2024). Habituation as optimal filtering.
  *iScience*, 27, 110523.
- Gershman, S. J. (2025). Bridging computation and representation in
  associative learning. *Computational Brain & Behavior*, 8, 377-391.
- Gibbon, J., Baldock, M., Locurto, C., Gold, L., & Terrace, H.
  (1977). Trial and intertrial durations in autoshaping. *Journal of
  Experimental Psychology: Animal Behavior Processes*, 3, 264-284.
- Gibbon, J., & Balsam, P. (1981). Spreading association in time. In
  C. M. Locurto, H. S. Terrace, & J. Gibbon (Eds.), *Autoshaping
  and Conditioning Theory* (pp. 219-253). Academic Press.
- Harris, J. A., & Gallistel, C. (2024). Information, certainty, and
  learning. *eLife*, 13.
- Rescorla, R. A., & Wagner, A. R. (1972). A theory of Pavlovian
  conditioning: Variations in the effectiveness of reinforcement and
  nonreinforcement. In A. Black & W. Prokasy (Eds.), *Classical
  Conditioning II* (pp. 64-99). Appleton-Century-Crofts.

---

## Appendix A: File Structure

```
conditioning/
+-- __init__.py       Package marker
+-- core.py           Layer 1-2: rate estimator, RW model, event builders,
|                     GenJAX model, validation
+-- simulations.py    Layer 3: all simulations (Figs 2-9)
+-- figs.py           I/O boundary: matplotlib rendering
+-- main.py           CLI entry point: --all, --fig N, --validate
+-- data/             Experimental datasets from Gershman's repository
|   +-- GibbonBalsam81.csv
|   +-- Balsam24.csv
|   +-- HarrisGallistel24.csv
+-- figs/             Generated figures (PNG)
```

## Appendix B: Running the Code

```bash
# Generate all figures
python -m conditioning.main --all

# Generate a specific figure
python -m conditioning.main --fig 5

# Validate GenJAX IS vs analytical posterior
python -m conditioning.main --validate
```

## Appendix C: Comparison of Functional Forms

The two informativeness laws compared in Figure 6:

```
Gallistel & Harris (2024):   R* = k / (C/T - 1)  =  k / (I/T)
Gershman (2025):             R* = k / (C/T)

On log scale:
  log(R*) = log(k) - log(C/T - 1)    vs
  log(R*) = log(k) - log(C/T)

These diverge at low informativeness (C/T close to 1):
  C/T = 1.5:  log(0.5) = -0.69  vs  log(1.5) = 0.41
  C/T = 10:   log(9)   =  2.20  vs  log(10)  = 2.30
  C/T = 100:  log(99)  =  4.60  vs  log(100) = 4.61

At high informativeness, the two forms are nearly identical.
The empirical data (which spans low to high informativeness)
consistently favors the C/T form.
```
